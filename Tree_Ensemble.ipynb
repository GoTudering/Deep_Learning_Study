{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tree_Ensemble.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPms7BDKJ0gDp2CpwkwaIDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GoTudering/Deep_Learning_Study/blob/main/Tree_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK90NObYlKcZ"
      },
      "source": [
        "'''\n",
        "정형 데이터와 비정형 데이터\n",
        "\n",
        "정형 데이터: 어떤 구조로 되어있어 CSV나 데이터베이스 엑셀에 저장하기 쉬움\n",
        "비정형 데이터: 정형 데이터의 반대. 덱스트 데이터, 사진, 디지털 음악 등이 있음\n",
        "\n",
        "머신러닝은 정형 데이터에 잘 맞음 비정형 데이터는 너무 복잡하여 신경망 학습이 필요함\n",
        "정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 앙상블 학습ensemble learning임\n",
        "이 알고리즘은 대부분 결정 트리를 기반으로 만들어져있음\n",
        "\n",
        "랜덤 포레스트는 앙상블 학습의 대표주자 중 하나로 앙상블 학습을 적용할 때 가장 먼저 랜덤 포레스트를 시도해보는 것이 좋음\n",
        "랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만듦. 그 방법은 훈련 데이터에서 랜덤하게 샘플을 추출하여(중복가능) 훈련데이터를 만듦\n",
        "이를 부트스트램 샘플bootstrap sample이라고 함(훈련 세트 크기 만큼 추출하기에 그 크기는 같음)\n",
        "또한 각 노드를 분할할 때 전체 특성 중 일부 특성을 무작위로 고른 다음 최선의 분할을 찾음\n",
        "분류모델인 RandomForestClassifier은 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택하고 회귀 모델인 RandomForestRegressor는 전체 특성을 사용함\n",
        "사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 훈련함\n",
        "분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼고 회귀일 때는 단순히 각 트리의 예측을 평균함\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "data=wine[['alcohol','sugar','pH']].to_numpy()\n",
        "target=wine[['class']].to_numpy()\n",
        "train_input, test_input, train_target, test_target = train_test_split(data,target,test_size=0.2,random_state=42)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UojYbpXelBnp",
        "outputId": "f3337ef2-2572-4c01-bb11-58d6f7afc976"
      },
      "source": [
        "#cross_validate()함수를 사용해 교차 검증\n",
        "#RandomForestClassifier은 기본적으로 100개의 결정 트리를 사용하므로 n_jobs매개변수를 -1로 지정하여 모든 CPU코어를 사용하는 것이 좋음\n",
        "#cross_validate()함수의 n_jobs 매개변수도 -1로 지정하여 최대한 병렬로 교차 검증을 수행\n",
        "#return_train_score 매개변수를 True로 지정하면 검증 점수뿐만 아니라 훈련 세트에 대한 점수도 같이 반환함(기본값은 False임)\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_jobs=-1,random_state=42)\n",
        "scores = cross_validate(rf,train_input,train_target,\n",
        "                        return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbumLEUynCOC",
        "outputId": "7052541d-6e19-425f-e3cf-87727b7d37ea"
      },
      "source": [
        "#랜덤 포레스트는 결정 트리의 앙상블이기 때문에 DecisionTreeClassifier가 제공하는 중요한 매개변수를 모두 제공함\n",
        "#criterion,max_depth,max_features,min_samples_split,min_impurity_decrease,min_samples_leaf\n",
        "#또한 결정 트리의 큰 장점 중 하나인 특성 중요도를 계산함\n",
        "rf.fit(train_input,train_target)\n",
        "print(rf.feature_importances_)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGIGBS7poFlV",
        "outputId": "dd5294a5-9cfb-474b-e135-90a6f8977a5e"
      },
      "source": [
        "#RandomForestClassifier에는 자체적으로 모델을 평가하는 점수를 얻을 수 있음\n",
        "#부트스트램 샘플에 포함되지 않고 남은 샘플이 있음 이를 OOB(out of bag)샘플 이라고함\n",
        "#이 샘플로 평가가능\n",
        "#이 점수를 얻으려면 RandomForestClassifier클래스의 oob_score 매개변수를 True로 지정해야함(기본적으로 False)\n",
        "#이렇게 하면 랜덤 포레스트는 각 결정 트리의 OOB점수를 평균하여 출력함\n",
        "rf=RandomForestClassifier(oob_score=True,n_jobs=-1,random_state=42)\n",
        "rf.fit(train_input,train_target)\n",
        "print(rf.oob_score_)\n",
        "#OOB점수를 사용하면 교차 검증을 대신할 수 있으서 결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있음"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBdTec1MpBqs",
        "outputId": "8b1e0f13-51f0-436b-81fb-20ebbd1089c1"
      },
      "source": [
        "#엑스트라 트리 Extra Trees\n",
        "#랜덤 포레스트ㅘ 매우 비슷. 기본적으로 100개의 결정트리를 훈련하고 결정 트리가 제공하는 대부분의 매개변수를 지원함\n",
        "#전체 특성 중 일부 특성을 랜덤하게 선택하여 사용\n",
        "#부트스트랩 샘플을 사용하지 않음 결정 트리를 만들 때 전체 훈련 세트를 사용\n",
        "#대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할함\n",
        "#엑스트라 트리가 사용하는 결정 트리가 바로 DecisionTreeClassifier 의 splitter 매개변수가 'random'인 결정트리임\n",
        "#하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만 많은 트리를 앙상블 하기에 과대적합을 먹고 검증 세트의 점수를 높임\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "et=ExtraTreesClassifier(n_jobs=-1,random_state=42)\n",
        "scores = cross_validate(et,train_input,train_target,return_train_score=True,n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQSN2UUcs4Sb",
        "outputId": "84b0bd8a-e4ed-4797-9e4b-3225f7112a5e"
      },
      "source": [
        "#특성 중요도도 제공\n",
        "et.fit(train_input,train_target)\n",
        "print(et.feature_importances_)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCyK4_O2tI5E",
        "outputId": "a1c37c24-e7f5-490e-86f0-ca3d4c00769e"
      },
      "source": [
        "#그레이디언트 부스팅 gradient boosting은 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식\n",
        "#GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용함\n",
        "#깊이가 얕아 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있음\n",
        "#경사 하강법을 사용하여 트리를 앙상블에 추가함\n",
        "#분류에서는 로지스틱 손실 함수를 사용하고 회귀에서는 평균 제곱 오차 함수를 사용함\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb=GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb,train_input,train_target,return_train_score=True,n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiIdIIPvyIEw",
        "outputId": "86088c52-b5d4-44f0-e145-42b3782f5c6c"
      },
      "source": [
        "gb=GradientBoostingClassifier(n_estimators=500,learning_rate=0.2,random_state=42)\n",
        "scores = cross_validate(gb,train_input,train_target,return_train_score=True,n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1-kOn0WzJWB",
        "outputId": "a4a6ced2-1c1d-4f03-e5b7-8df515c7344c"
      },
      "source": [
        "#그레이디언트 부스팅도 특성 중요도를 제공함\n",
        "gb.fit(train_input,train_target)\n",
        "print(gb.feature_importances_)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgKQGniN5hC6",
        "outputId": "c718b90f-c215-423b-b6ab-f2ede6c9ca67"
      },
      "source": [
        "#트리 훈련에 사용할 훈련 세트의 비율을 정하는 subsample 매개변수가 있음\n",
        "#기본값은 1 이보다 작으면 훈련 세트의 일부를 사용함\n",
        "#이는 마치 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는 확률적 경사 하강법이나 미니배치 경사 하강법과 비슷\n",
        "\n",
        "#일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있지만 순서대로 트리를 추가하기 때문에 훈련속도도 느림(n_jobs매개변수가 없음)\n",
        "\n",
        "#그레이디언트 부스팅의 속도와 성능을 더욱 개선한 것이 히스토그램 기반 그레이디언트 부스팅임\n",
        "\n",
        "#히스토그램 기반 그레이디언트 부스팅 Histogram-based Gradient Boosting 은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘임\n",
        "#입력 특성을 256개의 구간으로 나눔 -> 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있음\n",
        "#구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용하기에 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없음\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target,\n",
        "                        return_train_score=True)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejbH_XmJ9jhp",
        "outputId": "70a85792-4325-4766-d7f3-271079ca341c"
      },
      "source": [
        "#히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 계산하기 위해 permutation_importance()함수를 사용\n",
        "#특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여 어떤 특성이 중요한지 계산\n",
        "#n_repeats 매개변수는 랜덤하게 섞을 횟수를 지정(기본값은 5)\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input,train_target)\n",
        "result=permutation_importance(hgb,train_input,train_target,\n",
        "                              n_repeats=10,random_state=42,n_jobs=-1)\n",
        "print(result.importances_mean)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYakpEfA-WgJ",
        "outputId": "7bf43d03-f9ca-451d-bed3-42949071f36d"
      },
      "source": [
        "result=permutation_importance(hgb,test_input,test_target,\n",
        "                              n_repeats=10,random_state=42,n_jobs=-1)\n",
        "print(result.importances_mean)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY6TsmnI-kQx",
        "outputId": "f897abd0-94d9-4359-de9c-f92ae6c891da"
      },
      "source": [
        "hgb.score(test_input,test_target)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3s6HChO-oJM",
        "outputId": "8c1da9cf-2104-4bd7-e7db-43aca326de23"
      },
      "source": [
        "#XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(tree_method = 'hist',random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target,\n",
        "                        return_train_score=True)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8824322471423747 0.8726214185237284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnTPh-1hCXR2",
        "outputId": "e30f73cc-3cfb-4b9f-c555-586388c5bab5"
      },
      "source": [
        "#LightGBM\n",
        "from lightgbm import LGBMClassifier\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target,\n",
        "                        return_train_score=True,n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9338079582727165 0.8789710890649293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uioBPlpCn0j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}